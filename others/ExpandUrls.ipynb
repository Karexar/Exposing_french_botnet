{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from url_parser import *\n",
    "sys.path.append('/Volumes/GoogleDrive/Mon Drive/Python-helpers')\n",
    "from save_obj import *\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the urls shared on twitter have the format 't.co/'. The problem is that two urls with this format may lead to the same url at the end. Therefore, this format does not allow us to spot users that share the same url.  \n",
    "\n",
    "In order to retrieve the end urls, we will need to fetch the tweet_id associated with each tweet that has a t.co/ url. \n",
    "\n",
    "Then, we will use twarc to fetch each tweet corresponding to the tweet_id list we have. By fetching the tweet, we obtain a json file which contain the expanded url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 12.3 s, total: 1min 32s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.isfile(\"data/df_url.csv\"):\n",
    "    data_manager = DataManager()\n",
    "    data_manager.create_url_df()\n",
    "    \n",
    "df = pd.read_csv(\"data/df_url.csv\", encoding='utf-8', engine='python')\n",
    "df.dropna(inplace=True)\n",
    "df = df.drop(columns=[\"author_id\"])\n",
    "df[\"natural_key\"] = df.natural_key.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse the body to get the urls\n",
    "df[\"urls\"] = df[\"body\"].map(get_urls_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5154253, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>natural_key</th>\n",
       "      <th>body</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1104750238932365313</td>\n",
       "      <td>\"Mon gilet jaune, je l'ai brûlé\" : engagés de ...</td>\n",
       "      <td>[https://t.co/ixm2EDsStN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1128316229297557504</td>\n",
       "      <td>Pour ceux qui ne croient pas aux mouvements de...</td>\n",
       "      <td>[https://t.co/oicTarPgUJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1132242959183880192</td>\n",
       "      <td>Gilets Jaunes, une répression d'Etat | Documen...</td>\n",
       "      <td>[https://t.co/neGn8yleNq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1150334645160095744</td>\n",
       "      <td>Mdr, quelqu'un peut leur dire que c'est hasbee...</td>\n",
       "      <td>[https://t.co/tG2KyoGD5j]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1170185247876636678</td>\n",
       "      <td>Tarbes : les lycéens dans la rue, solidaires d...</td>\n",
       "      <td>[https://t.co/1mjkPwlici]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           natural_key                                               body  \\\n",
       "0  1104750238932365313  \"Mon gilet jaune, je l'ai brûlé\" : engagés de ...   \n",
       "2  1128316229297557504  Pour ceux qui ne croient pas aux mouvements de...   \n",
       "3  1132242959183880192  Gilets Jaunes, une répression d'Etat | Documen...   \n",
       "4  1150334645160095744  Mdr, quelqu'un peut leur dire que c'est hasbee...   \n",
       "5  1170185247876636678  Tarbes : les lycéens dans la rue, solidaires d...   \n",
       "\n",
       "                        urls  \n",
       "0  [https://t.co/ixm2EDsStN]  \n",
       "2  [https://t.co/oicTarPgUJ]  \n",
       "3  [https://t.co/neGn8yleNq]  \n",
       "4  [https://t.co/tG2KyoGD5j]  \n",
       "5  [https://t.co/1mjkPwlici]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the tweet that contains at least one url\n",
    "df = df[df['urls'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5154253, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We want one url on each row, with the corresponding tweet id. \n",
    "# First we take only the rows where there is only one url\n",
    "df_1 = df[df['urls'].map(len) == 1][['natural_key', 'urls']].copy()\n",
    "df_1['urls'] = df_1['urls'].map(lambda x: x[0])\n",
    "df_1.rename(columns={'urls':'url'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we take the rest\n",
    "df_2 = df[df['urls'].map(len) > 1][['natural_key', 'urls']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df_1.shape[0]+df_2.shape[0]==df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 757 ms, total: 1min 21s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# For all tweets that contains more than one url, flatten the list of urls to have \n",
    "# only one url per line. We duplicate the corresponding tweet id for all corresponding\n",
    "# lines\n",
    "df_2 = (df_2.set_index(['natural_key'])['urls'] # take column 'urls' as Series \n",
    "                                                # with index 'natural_key'\n",
    "            .apply(pd.Series) # transform to dataframe, for each row, take the \n",
    "                              # list in column 'urls' and put it on different\n",
    "                              # columns ('0', '1', '2' if 3 elements in list)\n",
    "                              # Fill with Nan in empty cells\n",
    "            .stack() # transform to Series with kind of hierarchical index. \n",
    "                     # the 1st is 'natural_key', the second (subgroup) is \n",
    "                     # the column index '0', '1', '2'...\n",
    "            .reset_index(level=1, drop=True) # drop subgroup index, duplicate\n",
    "                                             # 1st index 'natural_key' for each \n",
    "                                             # row in this group\n",
    "            .reset_index() # recreate dataframe with new index \n",
    "                           # and with column 'natural_key' and '0'\n",
    "            .rename(columns={0:'url'})) # rename column '0' to 'url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>natural_key</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1104750238932365313</td>\n",
       "      <td>https://t.co/ixm2EDsStN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1128316229297557504</td>\n",
       "      <td>https://t.co/oicTarPgUJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1132242959183880192</td>\n",
       "      <td>https://t.co/neGn8yleNq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1150334645160095744</td>\n",
       "      <td>https://t.co/tG2KyoGD5j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1170185247876636678</td>\n",
       "      <td>https://t.co/1mjkPwlici</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           natural_key                      url\n",
       "0  1104750238932365313  https://t.co/ixm2EDsStN\n",
       "2  1128316229297557504  https://t.co/oicTarPgUJ\n",
       "3  1132242959183880192  https://t.co/neGn8yleNq\n",
       "4  1150334645160095744  https://t.co/tG2KyoGD5j\n",
       "5  1170185247876636678  https://t.co/1mjkPwlici"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 59s, sys: 2.28 s, total: 7min 1s\n",
      "Wall time: 7min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now we only need one tweet id per url\n",
    "df_ids = (df.groupby('url')['natural_key']\n",
    "                                .apply(lambda x: list(x)[0])\n",
    "                                .reset_index(name='tweet_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3646789"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of unique tweet id\n",
    "ids = list(df_ids.tweet_id.unique())\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the ids in different files to keep intermediate results in case of crash. \n",
    "for i in range(8):\n",
    "    with open(\"ids\" + str(i) + \".txt\", \"w\") as f:\n",
    "        if i == 7:\n",
    "            for x in ids[7*500000:]:\n",
    "                f.write(x + \"\\n\")\n",
    "        else:\n",
    "            for x in ids[i*500000:(i+1)*500000]:\n",
    "                f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then use twarc to hydrate the tweet id and get the corresponding tweet with \n",
    "# all informations, including the expanded urls we are interested in.\n",
    "\n",
    "# twarc hydrate ids0.txt > tweets0.jsonl\n",
    "# same for 0 to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we read the json files. Each line represents a tweet in json format, which\n",
    "# can be converted in nested python dict. We are looking for url elements which appear\n",
    "# along with expanded_url (or simply 'expanded') element. \n",
    "# We build a dict mapping each url to the corresponding expanded_url. \n",
    "url_to_expanded = dict()\n",
    "\n",
    "def parse_url_json(e):\n",
    "    if type(e) is dict:\n",
    "        if \"url\" in e and \"expanded_url\" in e:\n",
    "            url_to_expanded[e[\"url\"]] = e[\"expanded_url\"]\n",
    "        elif \"url\" in e and \"expanded\" in e:\n",
    "            url_to_expanded[e[\"url\"]] = e[\"expanded\"]\n",
    "        for x in e:\n",
    "            #print(str(x) + \"  (\"+ str(type(e[x])) +\")\")\n",
    "            parse_url_json(e[x]) \n",
    "    elif type(e) is list:\n",
    "        for x in e:\n",
    "            if type(x) is dict:\n",
    "                parse_url_json(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3623356"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_to_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "1\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "2\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "3\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "4\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "5\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "6\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "7\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "CPU times: user 10min 2s, sys: 1min 7s, total: 11min 10s\n",
      "Wall time: 13min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j = 0\n",
    "for idx in list(range(8)):\n",
    "    print(idx)\n",
    "    for line in open('tweets' + str(idx) + '.jsonl', 'r'):\n",
    "        if j%100000 == 0:\n",
    "            print(j)\n",
    "        jfile = json.loads(line)\n",
    "        parse_url_json(jfile)\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DONT USE THIS : the recursive function is better\n",
    "if False:\n",
    "    #url_to_expanded = dict()\n",
    "    j = 0\n",
    "    for idx in list(range(8)):\n",
    "        print(idx)\n",
    "        for line in open('tweets' + str(idx) + '.jsonl', 'r'):\n",
    "            if j%100000 == 0:\n",
    "                print(j)\n",
    "            jfile = json.loads(line)\n",
    "\n",
    "            if \"entities\" in jfile and \"urls\" in jfile[\"entities\"]:\n",
    "                urls = jfile[\"entities\"][\"urls\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if (\"entities\" in jfile and \n",
    "                \"url\" in jfile[\"entities\"] and \n",
    "                \"urls\" in jfile[\"entities\"][\"url\"]):\n",
    "                urls = jfile[\"entities\"][\"url\"][\"urls\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if \"entities\" in jfile and \"media\" in jfile[\"entities\"]:\n",
    "                media = jfile[\"entities\"][\"media\"]\n",
    "                if len(media) > 0:\n",
    "                    for x in media:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if \"extended_entities\" in jfile and \"media\" in jfile[\"extended_entities\"]:\n",
    "                media = jfile[\"extended_entities\"][\"media\"]\n",
    "                if len(media) > 0:\n",
    "                    for x in media:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if (\"user\" in jfile and \n",
    "                \"entities\" in jfile[\"user\"] and \n",
    "                \"urls\" in jfile[\"user\"][\"entities\"]):\n",
    "                urls = jfile[\"user\"][\"entities\"][\"urls\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if (\"user\" in jfile and \n",
    "                \"entities\" in jfile[\"user\"] and \n",
    "                \"description\" in jfile[\"user\"][\"entities\"] and \n",
    "                \"urls\" in jfile[\"user\"][\"entities\"][\"description\"]):\n",
    "                urls = jfile[\"user\"][\"entities\"][\"description\"][\"urls\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if (\"user\" in jfile and \n",
    "                \"entities\" in jfile[\"user\"] and \n",
    "                \"url\" in jfile[\"user\"][\"entities\"] and \n",
    "                \"urls\" in jfile[\"user\"][\"entities\"][\"url\"]):\n",
    "                urls = jfile[\"user\"][\"entities\"][\"url\"][\"urls\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "\n",
    "            if \"quoted_status_permalink\" in jfile:\n",
    "                elem = jfile[\"quoted_status_permalink\"]\n",
    "                if elem is not None: \n",
    "                    url_to_expanded[elem[\"url\"]] = elem[\"expanded\"]\n",
    "\n",
    "            if (\"quoted_status\" in jfile and \n",
    "                \"entities\" in jfile[\"quoted_status\"] and \n",
    "                \"urls\" in jfile[\"quoted_status\"][\"entities\"]):\n",
    "                urls = jfile[\"quoted_status\"][\"entities\"][\"urls\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "            if (\"quoted_status\" in jfile and \n",
    "                \"entities\" in jfile[\"quoted_status\"] and \n",
    "                \"media\" in jfile[\"quoted_status\"][\"entities\"]):\n",
    "                urls = jfile[\"quoted_status\"][\"entities\"][\"media\"]\n",
    "                if len(urls) > 0:\n",
    "                    for x in urls:\n",
    "                        url_to_expanded[x[\"url\"]] = x[\"expanded_url\"]\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://sur.laprovence.com/cUP-f'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_to_expanded['https://t.co/TH66pjd3ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://twitter.com/snae_fr/status/1089606516540391424/video/1'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_to_expanded[\"https://t.co/V7tB2dqoZJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(url_to_expanded, \"data/\", \"url_to_expanded\")\n",
    "#url_to_expanded = load_obj(\"\", \"url_to_expanded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
